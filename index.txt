GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]Rank 3: Before barrier
rank: 3
world size: 4
Rank 1: Before barrier
rank: 1
world size: 4
Rank 2: Before barrier
rank: 2
world size: 4
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 178, in custom_barrier
[rank3]:     with dist.timeout(datetime.timedelta(seconds=0.1)):
[rank3]: AttributeError: module 'torch.distributed' has no attribute 'timeout'

[rank3]: During handling of the above exception, another exception occurred:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 261, in <module>
[rank3]:     main(args)
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 253, in main
[rank3]:     trainer.fit(model, train_loader)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank3]:     self.fit_loop.run()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank3]:     self.advance()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank3]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank3]:     self._optimizer_step(batch_idx, closure)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank3]:     call._call_lightning_module_hook(
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank3]:     optimizer.step(closure=optimizer_closure)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank3]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank3]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank3]:     return optimizer.step(closure=closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank3]:     ret = func(self, *args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank3]:     loss = closure()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank3]:     closure_result = closure()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank3]:     self._result = self.closure(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank3]:     step_output = self._step_fn()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank3]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank3]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank3]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank3]:     out = method(*_args, **_kwargs)
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 207, in training_step
[rank3]:     custom_barrier(timeout=timedelta(seconds=1))
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 181, in custom_barrier
[rank3]:     except (RuntimeError, dist.TimeoutError):
[rank3]: AttributeError: module 'torch.distributed' has no attribute 'TimeoutError'
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 178, in custom_barrier
[rank2]:     with dist.timeout(datetime.timedelta(seconds=0.1)):
[rank2]: AttributeError: module 'torch.distributed' has no attribute 'timeout'

[rank2]: During handling of the above exception, another exception occurred:

[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 261, in <module>
[rank2]:     main(args)
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 253, in main
[rank2]:     trainer.fit(model, train_loader)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank2]:     self.fit_loop.run()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank2]:     self.advance()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank2]:     self.epoch_loop.run(self._data_fetcher)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank2]:     self.advance(data_fetcher)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank2]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank2]:     self._optimizer_step(batch_idx, closure)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank2]:     call._call_lightning_module_hook(
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank2]:     optimizer.step(closure=optimizer_closure)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank2]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank2]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank2]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank2]:     return optimizer.step(closure=closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank2]:     loss = closure()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank2]:     closure_result = closure()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank2]:     self._result = self.closure(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank2]:     step_output = self._step_fn()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank2]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank2]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank2]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank2]:     out = method(*_args, **_kwargs)
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 207, in training_step
[rank2]:     custom_barrier(timeout=timedelta(seconds=1))
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 181, in custom_barrier
[rank2]:     except (RuntimeError, dist.TimeoutError):
[rank2]: AttributeError: module 'torch.distributed' has no attribute 'TimeoutError'
[rank1]: Traceback (most recent call last):
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 178, in custom_barrier
[rank1]:     with dist.timeout(datetime.timedelta(seconds=0.1)):
[rank1]: AttributeError: module 'torch.distributed' has no attribute 'timeout'

[rank1]: During handling of the above exception, another exception occurred:

[rank1]: Traceback (most recent call last):
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 261, in <module>
[rank1]:     main(args)
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 253, in main
[rank1]:     trainer.fit(model, train_loader)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank1]:     self.fit_loop.run()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank1]:     self.advance()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank1]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank1]:     self._optimizer_step(batch_idx, closure)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank1]:     call._call_lightning_module_hook(
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank1]:     optimizer.step(closure=optimizer_closure)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank1]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank1]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank1]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank1]:     return optimizer.step(closure=closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank1]:     loss = closure()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank1]:     closure_result = closure()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank1]:     self._result = self.closure(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank1]:     step_output = self._step_fn()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank1]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank1]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank1]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank1]:     out = method(*_args, **_kwargs)
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 207, in training_step
[rank1]:     custom_barrier(timeout=timedelta(seconds=1))
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 181, in custom_barrier
[rank1]:     except (RuntimeError, dist.TimeoutError):
[rank1]: AttributeError: module 'torch.distributed' has no attribute 'TimeoutError'
Rank 0: Before barrier
rank: 0
world size: 4
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 178, in custom_barrier
[rank0]:     with dist.timeout(datetime.timedelta(seconds=0.1)):
[rank0]: AttributeError: module 'torch.distributed' has no attribute 'timeout'

[rank0]: During handling of the above exception, another exception occurred:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 261, in <module>
[rank0]:     main(args)
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 253, in main
[rank0]:     trainer.fit(model, train_loader)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank0]:     self.advance()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank0]:     return optimizer.step(closure=closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank0]:     loss = closure()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank0]:     closure_result = closure()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank0]:     self._result = self.closure(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank0]:     step_output = self._step_fn()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank0]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank0]:     out = method(*_args, **_kwargs)
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 207, in training_step
[rank0]:     custom_barrier(timeout=timedelta(seconds=1))
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 181, in custom_barrier
[rank0]:     except (RuntimeError, dist.TimeoutError):
[rank0]: AttributeError: module 'torch.distributed' has no attribute 'TimeoutError'
Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]

[rank0]:[W901 04:06:07.093496696 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4

^CTraceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 154, in <module>
    import pytorch_lightning as pl
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/__init__.py", line 27, in <module>
    from pytorch_lightning.callbacks import Callback  # noqa: E402
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/__init__.py", line 14, in <module>
    from pytorch_lightning.callbacks.batch_size_finder import BatchSizeFinder
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/batch_size_finder.py", line 26, in <module>
    from pytorch_lightning.callbacks.callback import Callback
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/callback.py", line 22, in <module>
    from pytorch_lightning.utilities.types import STEP_OUTPUT
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/utilities/types.py", line 41, in <module>
    from torchmetrics import Metric
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/__init__.py", line 23, in <module>
    from torchmetrics import functional  # noqa: E402
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/__init__.py", line 14, in <module>
    from torchmetrics.functional.audio._deprecated import _permutation_invariant_training as permutation_invariant_training
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/audio/__init__.py", line 14, in <module>
    from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/audio/pit.py", line 22, in <module>
    from torchmetrics.utilities import rank_zero_warn
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/utilities/__init__.py", line 14, in <module>
    from torchmetrics.utilities.checks import check_forward_full_state_property
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/utilities/checks.py", line 25, in <module>
    from torchmetrics.metric import Metric
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/metric.py", line 42, in <module>
    from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE, plot_single_or_multi_val
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/utilities/plot.py", line 27, in <module>
    import matplotlib.pyplot as plt
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/pyplot.py", line 66, in <module>
    from matplotlib.figure import Figure, FigureBase, figaspect
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/figure.py", line 2154, in <module>
    class SubFigure(FigureBase):
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/artist.py", line 150, in __init_subclass__
    cls._update_set_signature_and_docstring()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/artist.py", line 178, in _update_set_signature_and_docstring
    + kwdoc(cls))
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/artist.py", line 1858, in kwdoc
    'Properties:\n' + '\n'.join(ai.pprint_setters(leadingspace=4)))
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/artist.py", line 1619, in pprint_setters
    accepts = self.get_valid_values(prop)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/matplotlib/artist.py", line 1486, in get_valid_values
    docstring = inspect.getdoc(func)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/inspect.py", line 735, in getdoc
    if doc is None:
KeyboardInterrupt

(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]Rank 2: Before barrier
Rank 3: Before barrier
rank: 2
world size: 4
rank: 3
world size: 4
Rank 1: Before barrier
rank: 1
world size: 4
Rank 0: Before barrier
rank: 0
world size: 4
[rank3]: Traceback (most recent call last):
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 263, in <module>
[rank3]:     main(args)
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 255, in main
[rank3]:     trainer.fit(model, train_loader)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank3]:     results = self._run_stage()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank3]:     self.fit_loop.run()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank3]:     self.advance()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank3]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank3]:     self._optimizer_step(batch_idx, closure)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank3]:     call._call_lightning_module_hook(
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank3]:     optimizer.step(closure=optimizer_closure)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank3]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank3]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank3]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank3]:     return optimizer.step(closure=closure, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank3]:     out = func(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank3]:     ret = func(self, *args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank3]:     loss = closure()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank3]:     closure_result = closure()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank3]:     self._result = self.closure(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank3]:     step_output = self._step_fn()
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank3]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank3]:     output = fn(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank3]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank3]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank3]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank3]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank3]:     out = method(*_args, **_kwargs)
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 209, in training_step
[rank3]:     custom_barrier(timeout=timedelta(seconds=1))
[rank3]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 180, in custom_barrier
[rank3]:     dist.all_reduce(tensor, op=dist.ReduceOp.SUM, timeout=datetime.timedelta(seconds=0.1))
[rank3]: NameError: name 'datetime' is not defined
[rank2]: Traceback (most recent call last):
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 263, in <module>
[rank2]:     main(args)
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 255, in main
[rank2]:     trainer.fit(model, train_loader)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank2]:     call._call_and_handle_interrupt(
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank2]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank2]:     return function(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank2]:     self._run(model, ckpt_path=ckpt_path)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank2]:     results = self._run_stage()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank2]:     self.fit_loop.run()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank2]:     self.advance()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank2]:     self.epoch_loop.run(self._data_fetcher)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank2]:     self.advance(data_fetcher)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank2]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank2]:     self._optimizer_step(batch_idx, closure)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank2]:     call._call_lightning_module_hook(
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank2]:     optimizer.step(closure=optimizer_closure)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank2]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank2]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank2]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank2]:     return optimizer.step(closure=closure, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank2]:     out = func(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank2]:     ret = func(self, *args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank2]:     loss = closure()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank2]:     closure_result = closure()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank2]:     self._result = self.closure(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank2]:     step_output = self._step_fn()
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank2]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank2]:     output = fn(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank2]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank2]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank2]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank2]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank2]:     out = method(*_args, **_kwargs)
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 209, in training_step
[rank2]:     custom_barrier(timeout=timedelta(seconds=1))
[rank2]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 180, in custom_barrier
[rank2]:     dist.all_reduce(tensor, op=dist.ReduceOp.SUM, timeout=datetime.timedelta(seconds=0.1))
[rank2]: NameError: name 'datetime' is not defined
[rank1]: Traceback (most recent call last):
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 263, in <module>
[rank1]:     main(args)
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 255, in main
[rank1]:     trainer.fit(model, train_loader)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank1]:     call._call_and_handle_interrupt(
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank1]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank1]:     return function(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank1]:     self._run(model, ckpt_path=ckpt_path)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank1]:     results = self._run_stage()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank1]:     self.fit_loop.run()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank1]:     self.advance()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank1]:     self.epoch_loop.run(self._data_fetcher)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank1]:     self.advance(data_fetcher)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank1]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank1]:     self._optimizer_step(batch_idx, closure)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank1]:     call._call_lightning_module_hook(
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank1]:     optimizer.step(closure=optimizer_closure)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank1]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank1]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank1]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank1]:     return optimizer.step(closure=closure, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank1]:     out = func(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank1]:     ret = func(self, *args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank1]:     loss = closure()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank1]:     closure_result = closure()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank1]:     self._result = self.closure(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank1]:     step_output = self._step_fn()
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank1]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank1]:     output = fn(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank1]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank1]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank1]:     out = method(*_args, **_kwargs)
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 209, in training_step
[rank1]:     custom_barrier(timeout=timedelta(seconds=1))
[rank1]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 180, in custom_barrier
[rank1]:     dist.all_reduce(tensor, op=dist.ReduceOp.SUM, timeout=datetime.timedelta(seconds=0.1))
[rank1]: NameError: name 'datetime' is not defined
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 263, in <module>
[rank0]:     main(args)
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 255, in main
[rank0]:     trainer.fit(model, train_loader)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
[rank0]:     call._call_and_handle_interrupt(
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank0]:     return function(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
[rank0]:     self._run(model, ckpt_path=ckpt_path)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 987, in _run
[rank0]:     results = self._run_stage()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1033, in _run_stage
[rank0]:     self.fit_loop.run()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 205, in run
[rank0]:     self.advance()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 363, in advance
[rank0]:     self.epoch_loop.run(self._data_fetcher)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 140, in run
[rank0]:     self.advance(data_fetcher)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 250, in advance
[rank0]:     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 190, in run
[rank0]:     self._optimizer_step(batch_idx, closure)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 268, in _optimizer_step
[rank0]:     call._call_lightning_module_hook(
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 157, in _call_lightning_module_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/module.py", line 1303, in optimizer_step
[rank0]:     optimizer.step(closure=optimizer_closure)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 152, in step
[rank0]:     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 270, in optimizer_step
[rank0]:     optimizer_output = super().optimizer_step(optimizer, closure, model, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 239, in optimizer_step
[rank0]:     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 122, in optimizer_step
[rank0]:     return optimizer.step(closure=closure, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 484, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/optim/sgd.py", line 112, in step
[rank0]:     loss = closure()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/precision.py", line 108, in _wrap_closure
[rank0]:     closure_result = closure()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 144, in __call__
[rank0]:     self._result = self.closure(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 129, in closure
[rank0]:     step_output = self._step_fn()
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/automatic.py", line 318, in _training_step
[rank0]:     training_step_output = call._call_strategy_hook(trainer, "training_step", *kwargs.values())
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 309, in _call_strategy_hook
[rank0]:     output = fn(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 390, in training_step
[rank0]:     return self._forward_redirection(self.model, self.lightning_module, "training_step", *args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 642, in __call__
[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 635, in wrapped_forward
[rank0]:     out = method(*_args, **_kwargs)
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 209, in training_step
[rank0]:     custom_barrier(timeout=timedelta(seconds=1))
[rank0]:   File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 180, in custom_barrier
[rank0]:     dist.all_reduce(tensor, op=dist.ReduceOp.SUM, timeout=datetime.timedelta(seconds=0.1))
[rank0]: NameError: name 'datetime' is not defined
Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s]

[rank0]:[W901 04:10:24.355995885 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]Rank 1: Before barrier
rank: 1
world size: 4
Rank 2: Before barrier
rank: 2
world size: 4
Rank 3: Before barrier
rank: 3
world size: 4
Rank 0: Before barrier
rank: 0
world size: 4
Rank 0: Barrier completed successfully in 0.004868 seconds
Rank 1: Barrier completed successfully in 0.017645 seconds
Rank 3: Barrier completed successfully in 0.015301 seconds
Rank 2: Barrier completed successfully in 0.016871 seconds
Rank 0: After barrier
Rank 1: After barrier
Rank 2: After barrier
Rank 3: After barrier
Epoch 0:  67%|                                                               | 2/3 [00:00<
00:00,  9.53it/s, v_num=32]Rank 2: Before barrier epoch end
rank: 2
Rank 1: Before barrier epoch end
world size: 4
rank: 1
world size: 4
Rank 3: Before barrier epoch end
rank: 3
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 13.84it/s, v_num=32]Rank 0: Before barrier epoch end
rank: 0
world size: 4
Rank 0: Barrier completed successfully in 0.002434 seconds
Rank 1: Barrier completed successfully in 0.004849 seconds
Rank 2: Barrier completed successfully in 0.004851 seconds
Rank 0: After barrier epoch end
Rank 1: After barrier epoch end
Rank 3: Barrier completed successfully in 0.004854 seconds
Rank 2: After barrier epoch end
Rank 3: After barrier epoch end
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|| 3/3 [00:00<
00:00, 10.22it/s, v_num=32]
[rank0]:[W901 04:10:50.894528991 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]Rank 2: Before barrier
rank: 2
world size: 4
Rank 3: Before barrier
rank: 3
world size: 4
Rank 1: Before barrier
rank: 1
world size: 4
Rank 0: Before barrier
rank: 0
world size: 4
Rank 1: Barrier completed successfully in 0.010967 seconds
Rank 0: Barrier completed successfully in 0.004873 seconds
Rank 1: After barrier
Rank 3: Barrier completed successfully in 0.069207 seconds
Rank 2: Barrier completed successfully in 0.069424 seconds
Rank 3: After barrier
Rank 2: After barrier
Rank 0: After barrier
Epoch 0:  67%|                                                               | 2/3 [00:00<
00:00,  8.16it/s, v_num=33]Rank 2: Before barrier epoch end
rank: 2
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 11.52it/s, v_num=33]Rank 1: Before barrier epoch end
rank: 1
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 11.49it/s, v_num=33]Rank 0: Before barrier epoch end
rank: 0
world size: 4
Rank 3: Before barrier epoch end
rank: 3
world size: 4
Rank 0: Barrier completed successfully in 0.002431 seconds
Rank 2: Barrier completed successfully in 0.002430 seconds
Rank 1: Barrier completed successfully in 0.002429 seconds
Rank 0: After barrier epoch end
Rank 2: After barrier epoch end
Rank 1: After barrier epoch end
Rank 3: Barrier completed successfully in 0.001860 seconds
Rank 3: After barrier epoch end
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|| 3/3 [00:00<
00:00,  8.79it/s, v_num=33]
[rank0]:[W901 04:11:29.475254294 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
^CTraceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 154, in <module>
    import pytorch_lightning as pl
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/__init__.py", line 27, in <module>
    from pytorch_lightning.callbacks import Callback  # noqa: E402
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/__init__.py", line 14, in <module>
    from pytorch_lightning.callbacks.batch_size_finder import BatchSizeFinder
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/batch_size_finder.py", line 26, in <module>
    from pytorch_lightning.callbacks.callback import Callback
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/callbacks/callback.py", line 22, in <module>
    from pytorch_lightning.utilities.types import STEP_OUTPUT
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/utilities/types.py", line 41, in <module>
    from torchmetrics import Metric
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/__init__.py", line 23, in <module>
    from torchmetrics import functional  # noqa: E402
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/__init__.py", line 122, in <module>
    from torchmetrics.functional.text._deprecated import _bleu_score as bleu_score
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/text/__init__.py", line 50, in <module>
    from torchmetrics.functional.text.bert import bert_score
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torchmetrics/functional/text/bert.py", line 55, in <module>
    from transformers import AutoModel, AutoTokenizer
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/transformers/__init__.py", line 26, in <module>
    from . import dependency_versions_check
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/transformers/dependency_versions_check.py", line 16, in <module>
    from .utils.versions import require_version, require_version_core
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/transformers/utils/__init__.py", line 18, in <module>
    from huggingface_hub import get_full_repo_name  # for backward compatibility
  File "<frozen importlib._bootstrap>", line 1075, in _handle_fromlist
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/__init__.py", line 520, in __getattr__
    submod = importlib.import_module(submod_path)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/huggingface_hub/hf_api.py", line 45, in <module>
    import requests
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/requests/__init__.py", line 151, in <module>
    from . import packages, utils
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/requests/utils.py", line 52, in <module>
    from .cookies import cookiejar_from_dict
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 879, in exec_module
  File "<frozen importlib._bootstrap_external>", line 975, in get_code
  File "<frozen importlib._bootstrap_external>", line 1074, in get_data
KeyboardInterrupt

(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]before barrier
rank: 3
world size: 4
before barrier
rank: 2
world size: 4
before barrier
rank: 1
world size: 4
before barrier
rank: 0
world size: 4
Rank 0: Barrier completed successfully
Rank 3: Barrier completed successfully
Rank 1: Barrier completed successfully
Rank 2: Barrier completed successfully
after barrier
after barrier
after barrier
after barrier
Epoch 0:  67%|                                                               | 2/3 [00:00<
00:00,  9.67it/s, v_num=34]before barrier epoch end
before barrier epoch end
rank: 2
world size: 4
before barrier epoch end
rank: 1
rank: 3
world size: 4
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 14.30it/s, v_num=34]before barrier epoch end
rank: 0
world size: 4
Rank 0: Barrier completed successfully
Rank 3: Barrier completed successfully
Rank 2: Barrier completed successfully
after barrier epoch end
after barrier epoch end
after barrier epoch end
Rank 1: Barrier completed successfully
after barrier epoch end
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|| 3/3 [00:00<
00:00, 10.46it/s, v_num=34]
[rank0]:[W901 04:12:19.206514239 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 2 --num_gpus 4

/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 2 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 2 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 254, in <module>
    main(args)
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 246, in main
    trainer.fit(model, train_loader)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 943, in _run
    self.strategy.setup_environment()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 203, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/utilities/distributed.py", line 291, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1361, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 258, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 185, in _create_c10d_store
    return TCPStore(
torch.distributed.DistStoreError: Timed out after 3 seconds waiting for clients. 1/4 clients joined.
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4

Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
[E901 04:12:41.047916190 socket.cpp:957] [c10d] The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 254, in <module>
    main(args)
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 246, in main
    trainer.fit(model, train_loader)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 943, in _run
    self.strategy.setup_environment()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 203, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/utilities/distributed.py", line 291, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1361, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 258, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 185, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
[E901 04:12:41.103128314 socket.cpp:957] [c10d] The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 254, in <module>
    main(args)
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 246, in main
    trainer.fit(model, train_loader)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 943, in _run
    self.strategy.setup_environment()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 203, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/utilities/distributed.py", line 291, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1361, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 258, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 185, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
[E901 04:12:41.117705998 socket.cpp:957] [c10d] The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 254, in <module>
    main(args)
  File "/data/yingzi_ma/lean_project/ReProver/distributed_test.py", line 246, in main
    trainer.fit(model, train_loader)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 544, in fit
    call._call_and_handle_interrupt(
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 43, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 580, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 943, in _run
    self.strategy.setup_environment()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 154, in setup_environment
    self.setup_distributed()
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 203, in setup_distributed
    _init_dist_connection(self.cluster_environment, self._process_group_backend, timeout=self._timeout)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/utilities/distributed.py", line 291, in _init_dist_connection
    torch.distributed.init_process_group(torch_distributed_backend, rank=global_rank, world_size=world_size, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
    return func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
    func_return = func(*args, **kwargs)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 1361, in init_process_group
    store, rank, world_size = next(rendezvous_iterator)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 258, in _env_rendezvous_handler
    store = _create_c10d_store(master_addr, master_port, rank, world_size, timeout, use_libuv)
  File "/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/distributed/rendezvous.py", line 185, in _create_c10d_store
    return TCPStore(
torch.distributed.DistNetworkError: The client socket has timed out after 2s while trying to connect to (127.0.0.1, 33623).
^C
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4

/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]before barrier
rank: 1
world size: 4
before barrier
rank: 2
world size: 4
before barrier
rank: 3
world size: 4
before barrier
rank: 0
world size: 4
Rank 0: Barrier completed successfully
Rank 3: Barrier completed successfully
Rank 2: Barrier completed successfully
Rank 1: Barrier completed successfully
after barrier
after barrier
after barrier
after barrier
Epoch 0:  67%|                                                               | 2/3 [00:00<
00:00,  8.93it/s, v_num=35]before barrier epoch end
Epoch 0: 100%|| 3/3 [00:00<
00:00, 13.19it/s, v_num=35]before barrier epoch end
rank: 1
world size: 4
rank: 3
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 13.15it/s, v_num=35]before barrier epoch end
rank: 0
world size: 4
before barrier epoch end
rank: 2
world size: 4
Rank 1: Barrier completed successfully
Rank 0: Barrier completed successfully
Rank 3: Barrier completed successfully
after barrier epoch end
after barrier epoch end
after barrier epoch end
Rank 2: Barrier completed successfully
after barrier epoch end
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|| 3/3 [00:00<
00:00,  9.11it/s, v_num=35]
[rank0]:[W901 04:15:02.397400788 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ python distributed_test.py --timeout 30 --num_gpus 4
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python distributed_test.py --timeout 30 --num_gpus 4 ...
You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performa
nce. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

NCCL version 2.20.5+cuda12.4
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name  | Type   | Params
---------------------------------
0 | layer | Linear | 2
---------------------------------
2         Trainable params
0         Non-trainable params
2         Total params
0.000     Total estimated model params size (MB)
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set
 a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 0:   0%|
     | 0/3 [00:00<?, ?it/s]before barrier
rank: 1
world size: 4
before barrier
rank: 2
world size: 4
before barrier
rank: 3
world size: 4
before barrier
rank: 0
world size: 4
Rank 3: Barrier completed successfully
Rank 0: Barrier completed successfully
Rank 1: Barrier completed successfully
Rank 2: Barrier completed successfully
after barrier
after barrier
after barrier
after barrier
Epoch 0:  67%|                                                               | 2/3 [00:00<
00:00,  9.58it/s, v_num=36]before barrier epoch end
rank: 2
world size: 4
before barrier epoch end
rank: 1
world size: 4
Epoch 0: 100%|| 3/3 [00:00<
00:00, 14.15it/s, v_num=36]before barrier epoch end
rank: 0
world size: 4
before barrier epoch end
rank: 3
world size: 4
Rank 0: Barrier completed successfully
Rank 1: Barrier completed successfully
Rank 3: Barrier completed successfully
Rank 2: Barrier completed successfully
after barrier epoch end
after barrier epoch end
after barrier epoch end
after barrier epoch end
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|| 3/3 [00:00<
00:00, 10.39it/s, v_num=36]
[rank0]:[W901 04:15:21.540724099 ProcessGroupNCCL.cpp:1168] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_proce
ss_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint h
as always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
(ReProver) yingzi_ma@compute-permanent-node-834:~/lean_project/ReProver$ bash run_code.sh
Script executed from: /data/yingzi_ma/lean_project/ReProver
[2024-09-01 04:19:47,933] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-09-01 04:19:51.604 | INFO     | __main__:main:1232 - Running progressive training
2024-09-01 04:19:51.604 | INFO     | __main__:main:1238 - Configuring LeanDojo...
2024-09-01 04:19:51.609 | INFO     | generate_benchmark_lean4:configure_leandojo:317 - Current working directory: /data/yingzi_ma/lean_project/ReProver
2024-09-01 04:19:51.609 | INFO     | __main__:main:1240 - LeanDojo configured
2024-09-01 04:19:53.241 | INFO     | __main__:search_github_repositories:403 - Processing leanprover/lean4
2024-09-01 04:19:53.242 | INFO     | __main__:search_github_repositories:421 - Skipping leanprover/lean4 since it is a known repository
2024-09-01 04:19:53.242 | INFO     | __main__:search_github_repositories:403 - Processing leanprover-community/mathlib3
Cloning https://github.com/leanprover-community/mathlib3.git
Repo name: leanprover-community/mathlib3
2024-09-01 04:19:54.127 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/leanprover-community/mathlib3/65a1391a0106c9204fe45bc73a039f056558cb83/lean-toolchain failed. Retrying..
.
2024-09-01 04:19:55.166 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/leanprover-community/mathlib3/65a1391a0106c9204fe45bc73a039f056558cb83/lean-toolchain failed. Retrying..
.
2024-09-01 04:19:57.205 | INFO     | __main__:search_github_repositories:419 - Failed to clone leanprover-community/mathlib3 because of HTTP Error 404: Not Found
2024-09-01 04:19:57.205 | INFO     | __main__:search_github_repositories:403 - Processing leanprover-community/mathlib4
Cloning https://github.com/leanprover-community/mathlib4.git
Repo name: leanprover-community/mathlib4
2024-09-01 04:19:58.142 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.11.0-rc3
2024-09-01 04:20:13.505 | INFO     | __main__:search_github_repositories:403 - Processing uwdb/Cosette
Cloning https://github.com/uwdb/Cosette.git
Repo name: uwdb/Cosette
2024-09-01 04:20:14.203 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/uwdb/Cosette/c16b48ec7d278c717a9f1da3684f69621b3863f2/lean-toolchain failed. Retrying...
2024-09-01 04:20:15.242 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/uwdb/Cosette/c16b48ec7d278c717a9f1da3684f69621b3863f2/lean-toolchain failed. Retrying...
2024-09-01 04:20:17.283 | INFO     | __main__:search_github_repositories:419 - Failed to clone uwdb/Cosette because of HTTP Error 404: Not Found
2024-09-01 04:20:17.284 | INFO     | __main__:search_github_repositories:403 - Processing AndrasKovacs/smalltt
Cloning https://github.com/AndrasKovacs/smalltt.git
Repo name: AndrasKovacs/smalltt
2024-09-01 04:20:17.999 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/AndrasKovacs/smalltt/b5fc9b3747dcbf8c71ca9f976afb7b574b630996/lean-toolchain failed. Retrying...
2024-09-01 04:20:19.039 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/AndrasKovacs/smalltt/b5fc9b3747dcbf8c71ca9f976afb7b574b630996/lean-toolchain failed. Retrying...
2024-09-01 04:20:21.079 | INFO     | __main__:search_github_repositories:419 - Failed to clone AndrasKovacs/smalltt because of HTTP Error 404: Not Found
2024-09-01 04:20:21.080 | INFO     | __main__:search_github_repositories:403 - Processing dselsam/certigrad
Cloning https://github.com/dselsam/certigrad.git
Repo name: dselsam/certigrad
2024-09-01 04:20:21.773 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/dselsam/certigrad/c9a06e93f1ec58196d6d3b8563b29868d916727f/lean-toolchain failed. Retrying...
2024-09-01 04:20:22.813 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/dselsam/certigrad/c9a06e93f1ec58196d6d3b8563b29868d916727f/lean-toolchain failed. Retrying...
2024-09-01 04:20:24.853 | INFO     | __main__:search_github_repositories:419 - Failed to clone dselsam/certigrad because of HTTP Error 404: Not Found
2024-09-01 04:20:24.854 | INFO     | __main__:search_github_repositories:403 - Processing Kha/electrolysis
Cloning https://github.com/Kha/electrolysis.git
Repo name: Kha/electrolysis
2024-09-01 04:20:25.671 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/Kha/electrolysis/bd4655d91ca65856fb8373637ce3e7b17ee3a190/lean-toolchain failed. Retrying...
2024-09-01 04:20:26.711 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/Kha/electrolysis/bd4655d91ca65856fb8373637ce3e7b17ee3a190/lean-toolchain failed. Retrying...
2024-09-01 04:20:28.750 | INFO     | __main__:search_github_repositories:419 - Failed to clone Kha/electrolysis because of HTTP Error 404: Not Found
2024-09-01 04:20:28.751 | INFO     | __main__:search_github_repositories:403 - Processing ImperialCollegeLondon/formalising-mathematics
Cloning https://github.com/ImperialCollegeLondon/formalising-mathematics.git
Repo name: ImperialCollegeLondon/formalising-mathematics
2024-09-01 04:20:29.476 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/ImperialCollegeLondon/formalising-mathematics/d81ad67e36774eb68f242bcc1b88d33dacfd87bf/lean-toolchain fa
iled. Retrying...
2024-09-01 04:20:30.514 | DEBUG    | lean_dojo.utils:read_url:178 - Request to https://raw.githubusercontent.com/ImperialCollegeLondon/formalising-mathematics/d81ad67e36774eb68f242bcc1b88d33dacfd87bf/lean-toolchain fa
iled. Retrying...
2024-09-01 04:20:32.553 | INFO     | __main__:search_github_repositories:419 - Failed to clone ImperialCollegeLondon/formalising-mathematics because of HTTP Error 404: Not Found
2024-09-01 04:20:32.554 | INFO     | __main__:search_github_repositories:403 - Processing lecopivo/SciLean
Cloning https://github.com/lecopivo/SciLean.git
Repo name: lecopivo/SciLean
2024-09-01 04:20:33.272 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.11.0-rc2
Found 2 repositories
Using lambda = 0.1
Processing /data/yingzi_ma/lean_project/repos_new/leanprover-community/mathlib4
2024-09-01 04:20:48.661 | INFO     | __main__:retrieve_proof:1047 - Processing https://github.com/leanprover-community/mathlib4.git
2024-09-01 04:20:48.661 | INFO     | __main__:retrieve_proof:1063 - Found compatible commit 2b29e73438e240a427bcecc7c0fe19306beb1310 for https://github.com/leanprover-community/mathlib4.git
2024-09-01 04:20:48.661 | INFO     | __main__:retrieve_proof:1064 - Lean version: v4.8.0
2024-09-01 04:20:48.900 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.8.0
2024-09-01 04:21:04.158 | INFO     | __main__:retrieve_proof:1069 - Generating benchmark at /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:324 - Generating dataset to go into /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:331 - lean toolchain version: {'content': 'leanprover/lean4:v4.8.0\n'}
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:333 - lean version v: v4.8.0
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:334 - is supported: True
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:344 - lean path1 /data/yingzi_ma/.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:345 - lean path2 /.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.158 | INFO     | generate_benchmark_lean4:main:346 - lean path3 ~/.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.160 | INFO     | generate_benchmark_lean4:main:350 - Lean toolchain path 2 does not exist: /.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.160 | INFO     | generate_benchmark_lean4:main:352 - Lean toolchain path 3 does not exist: ~/.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.160 | INFO     | generate_benchmark_lean4:main:355 - Switched to Lean toolchain at: /data/yingzi_ma/.elan/toolchains/leanprover--lean4---4.8.0
2024-09-01 04:21:04.258 | INFO     | generate_benchmark_lean4:main:357 - lean --version: Lean (version 4.8.0, x86_64-unknown-linux-gnu, commit df668f00e6c0, Release)

2024-09-01 04:21:04.258 | INFO     | generate_benchmark_lean4:main:358 - repo: LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='2b29e73438e240a427bcecc7c0fe19306beb1310')
2024-09-01 04:21:04.258 | INFO     | generate_benchmark_lean4:main:360 - Configuring LeanDojo again...
2024-09-01 04:21:04.266 | INFO     | generate_benchmark_lean4:configure_leandojo:317 - Current working directory: /data/yingzi_ma/lean_project/ReProver
2024-09-01 04:21:04.266 | INFO     | generate_benchmark_lean4:main:362 - LeanDojo configured
2024-09-01 04:21:04.266 | INFO     | generate_benchmark_lean4:main:365 - Tracing the repo...
2024-09-01 04:21:04.269 | DEBUG    | lean_dojo.data_extraction.trace:get_traced_repo_path:87 - The traced repo is available in the cache.
2024-09-01 04:21:04.269 | INFO     | lean_dojo.data_extraction.trace:trace:116 - Loading the traced repo from /data/yingzi_ma/lean_project/.cache/lean_dojo/leanprover-community-mathlib4-2b29e73438e240a427bcecc7c0fe193
06beb1310/mathlib4
2024-09-01 04:21:07.674 | DEBUG    | lean_dojo.data_extraction.traced_data:load_from_disk:1123 - Loading 5514 traced XML files from /data/yingzi_ma/lean_project/.cache/lean_dojo/leanprover-community-mathlib4-2b29e7343
8e240a427bcecc7c0fe19306beb1310/mathlib4 with 31 workers
 37%|                                                 37%|                                                                  | 2055/5514 [
100%|| 5514/5514 [15:59<00:00,  5.75it/s]
2024-09-01 04:37:06.730 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='2b29e73438e240a427bce
cc7c0fe19306beb1310')
2024-09-01 04:37:08.134 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.6.0-rc1
2024-09-01 04:37:24.605 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.8.0-rc1
2024-09-01 04:37:40.454 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.7.0
2024-09-01 04:37:56.037 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:70 - url_to_repo("https://github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-09-01 04:37:57.178 | DEBUG    | lean_dojo.data_extraction.lean:url_to_repo:70 - url_to_repo("https://github.com/leanprover-community/import-graph.git") failed. Retrying...
2024-09-01 04:37:59.847 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for doc-gen4 main
2024-09-01 04:38:00.455 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for batteries main
2024-09-01 04:38:10.137 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for quote4 master
2024-09-01 04:38:10.529 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.10.0
2024-09-01 04:38:26.046 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for aesop master
2024-09-01 04:38:27.126 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.11.0-rc1
2024-09-01 04:38:42.520 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for ProofWidgets4 v0.0.36
2024-09-01 04:38:43.359 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4-cli main
2024-09-01 04:38:43.993 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.9.0
2024-09-01 04:38:59.833 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for import-graph main
2024-09-01 04:39:00.459 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover/doc-gen4', commit='83f718b9055972dce4f92f5b3917426
b91a0d2fe')
2024-09-01 04:39:01.043 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4 v4.9.0-rc1
Following Github server redirection from /repos/mhuisi/lean4-cli to /repositories/341363356
2024-09-01 04:39:18.112 | DEBUG    | lean_dojo.data_extraction.lean:_to_commit_hash:90 - Querying the commit hash for lean4-nightly nightly-2024-06-05
2024-09-01 04:39:19.640 | WARNING  | lean_dojo.data_extraction.lean:__post_init__:446 - LeanGitRepo(url='https://github.com/mhuisi/lean4-cli', commit='bf066c328bcff19aa93adf4d24c4e896c0d4eaca') relies on an unsupporte
d Lean version: fbb3055f82ff9a4bc82f6722f6e73225617342fd
2024-09-01 04:39:19.640 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/batteries', commit='a7fd140a94bbbfa40cf1
0839227bbb9e8492be2d')
2024-09-01 04:39:19.850 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/quote4', commit='d38fb94558af9957b8f479e
350841ce65a1ec42c')
2024-09-01 04:39:20.038 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/aesop', commit='6c89b6765913a0d727c10664
93c3f5dc51c9713e')
2024-09-01 04:39:20.498 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/ProofWidgets4', commit='e6b6247c61280c77
ade6bbf0bc3c66a44fe2e0c5')
2024-09-01 04:39:20.976 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover/lean4-cli', commit='2cf1030dc2ae6b3632c84a09350b67
5ef3e347d0')
2024-09-01 04:39:21.170 | DEBUG    | lean_dojo.data_extraction.lean:get_dependencies:499 - Querying the dependencies of LeanGitRepo(url='https://github.com/leanprover-community/import-graph', commit='0a756018b736ef4f4
bc3c9a45fc03cf97a0b1a0c')
2024-09-01 04:39:53.598 | DEBUG    | lean_dojo.data_extraction.traced_data:check_sanity:1009 - Checking the sanity of TracedRepo(repo=LeanGitRepo(url='https://github.com/leanprover-community/mathlib4', commit='2b29e73
438e240a427bcecc7c0fe19306beb1310'), dependencies={'lean4': LeanGitRepo(url='https://github.com/leanprover/lean4', commit='daa22187642d4cf6954c39a23eab20d8a8675416'), 'batteries': LeanGitRepo(url='https://github.com/l
eanprover-community/batteries', commit='021e272cb5cdcc82b7e1e760fe915ff2f64169ad'), 'Qq': LeanGitRepo(url='https://github.com/leanprover-community/quote4', commit='53156671405fbbd5402ed17a79bd129b961bd8d6'), 'aesop':
LeanGitRepo(url='https://github.com/leanprover-community/aesop', commit='53ba96ad7666d4a2515292974631629b5ea5dfee'), 'proofwidgets': LeanGitRepo(url='https://github.com/leanprover-community/ProofWidgets4', commit='e6b
6247c61280c77ade6bbf0bc3c66a44fe2e0c5'), 'Cli': LeanGitRepo(url='https://github.com/leanprover/lean4-cli', commit='2cf1030dc2ae6b3632c84a09350b675ef3e347d0'), 'doc-gen4': LeanGitRepo(url='https://github.com/leanprov
er/doc-gen4', commit='83f718b9055972dce4f92f5b3917426b91a0d2fe'), 'MD4Lean': LeanGitRepo(url='https://github.com/acmepjz/md4lean', commit='5e95f4776be5e048364f325c7e9d619bb56fb005'), 'UnicodeBasic': LeanGitRepo(url='h
ttps://github.com/fgdorais/lean4-unicode-basic', commit='9447739fe9714f8a091192bad5cd5e7b5a8ae1e4'), 'BibtexQuery': LeanGitRepo(url='https://github.com/dupuisf/BibtexQuery', commit='c138ab5c566c4268798b24bfcbea2f4c424
13cdd'), 'importGraph': LeanGitRepo(url='https://github.com/leanprover-community/import-graph', commit='0a756018b736ef4f4bc3c9a45fc03cf97a0b1a0c')}, root_dir=PosixPath('/data/yingzi_ma/lean_project/.cache/lean_dojo/le
anprover-community-mathlib4-2b29e73438e240a427bcecc7c0fe19306beb1310/mathlib4'))
2024-09-01 04:40:17.562 | INFO     | generate_benchmark_lean4:main:367 - Successfully traced the repo
2024-09-01 04:40:17.563 | WARNING  | generate_benchmark_lean4:main:372 - /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310 already exists. Using it instead.
2024-09-01 04:40:17.563 | INFO     | __main__:retrieve_proof:1077 - Finished generating benchmark at /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310
2024-09-01 04:40:34.047 | INFO     | dynamic_database:add_repository:583 - Attempting to add repository: https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
2024-09-01 04:40:34.047 | INFO     | dynamic_database:add_repository:586 - Added new repository: https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
100%|| 2421/2421 [00:00<00:00, 35098.72it/s]
100%|| 116251/116251 [01:44<00:00, 1110.44it/s]
100%|| 2421/2421 [00:00<00:00, 35819.79it/s]
2024-09-01 04:42:24.891 | INFO     | __main__:retrieve_proof:1115 - Before adding new repo:
2024-09-01 04:42:24.891 | INFO     | dynamic_database:print_database_contents:607 - Current database contents:
2024-09-01 04:42:24.891 | INFO     | dynamic_database:print_database_contents:609 -   - https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
2024-09-01 04:42:24.891 | INFO     | dynamic_database:add_repository:583 - Attempting to add repository: https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
2024-09-01 04:42:24.891 | INFO     | dynamic_database:add_repository:588 - Repository 'https://github.com/leanprover-community/mathlib4' with commit '2b29e73438e240a427bcecc7c0fe19306beb1310' already exists in the dat
abase.
2024-09-01 04:42:24.891 | INFO     | __main__:retrieve_proof:1118 - After adding new repo:
2024-09-01 04:42:24.892 | INFO     | dynamic_database:print_database_contents:607 - Current database contents:
2024-09-01 04:42:24.892 | INFO     | dynamic_database:print_database_contents:609 -   - https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
2024-09-01 04:42:45.787 | INFO     | __main__:retrieve_proof:1127 - Adding repo to repos_for_merged_dataset
2024-09-01 04:42:45.789 | INFO     | dynamic_database:generate_merged_dataset:399 - Merging selected repositories in the database:
2024-09-01 04:42:45.789 | INFO     | dynamic_database:generate_merged_dataset:401 -   - https://github.com/leanprover-community/mathlib4 (commit: 2b29e73438e240a427bcecc7c0fe19306beb1310)
2024-09-01 04:46:21.919 | WARNING  | dynamic_database:generate_merged_dataset:418 - /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310 already ex
ists. Removing it now.
2024-09-01 04:46:56.860 | INFO     | dynamic_database:generate_merged_dataset:422 - Exported proofs to /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe1930
6beb1310
2024-09-01 04:46:57.813 | INFO     | dynamic_database:generate_merged_dataset:425 - Merged and exported corpus to /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bce
cc7c0fe19306beb1310
2024-09-01 04:46:57.835 | INFO     | dynamic_database:generate_merged_dataset:428 - Exported traced files to /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0
fe19306beb1310
2024-09-01 04:46:57.838 | INFO     | dynamic_database:generate_merged_dataset:431 - Exported metadata to /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19
306beb1310
2024-09-01 04:46:57.870 | INFO     | __main__:find_latest_checkpoint:486 - Using the latest checkpoint: /data/yingzi_ma/lean_project/checkpoints_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe
19306beb1310_lambda_0.1_epoch=0-Recall@10_val=58.95.ckpt
2024-09-01 04:46:57.870 | INFO     | __main__:retrieve_proof:1139 - Found latest checkpoint: /data/yingzi_ma/lean_project/checkpoints_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb131
0_lambda_0.1_epoch=0-Recall@10_val=58.95.ckpt
2024-09-01 04:46:57.870 | INFO     | __main__:train_test_fisher:626 - Inside train_test_fisher
2024-09-01 04:46:57.870 | INFO     | __main__:train_test_fisher:627 - Starting training at epoch 0
[rank: 0] Seed set to 3407
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/utilities/cloud_io.py:57: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default
 pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more
details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be load
ed via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of
the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.4.0, which is newer than your current Light
ning version: v2.2.4
2024-09-01 04:47:02.651 | INFO     | __main__:train_test_fisher:651 - Loaded premise retriever at /data/yingzi_ma/lean_project/checkpoints_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306b
eb1310_lambda_0.1_epoch=0-Recall@10_val=58.95.ckpt and reset epoch count
2024-09-01 04:47:02.655 | INFO     | __main__:find_latest_fisher:496 - Using the latest Fisher Information Matrix: /data/yingzi_ma/lean_project/fisher_PT_single_repo_ewc/mathlib4_29dcec074de168ac2bf835a77ef68bbe069194
c5_fisher_info.pkl
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickl
e module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more detail
s). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via
 this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the lo
aded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(io.BytesIO(b))
2024-09-01 04:47:03.485 | INFO     | __main__:load_fisher_information:473 - Fisher Information successfully loaded.
2024-09-01 04:47:03.485 | INFO     | retrieval.model:set_fisher_info:62 - Fisher Information has been updated in the model.
2024-09-01 04:47:03.485 | INFO     | __main__:train_test_fisher:657 - Fisher Information Matrix loaded.
Data path: /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310/random
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This
behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
2024-09-01 04:47:03.594 | INFO     | common:__init__:208 - Building the corpus from /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310/corpus.jso
nl
100%|| 116251/116251 [00:27<00:00, 4266.25it/s]
2024-09-01 04:48:02.581 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cache /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0
fe19306beb1310/random/cache_train/cached_data.pkl
Training dataset size: 359583
100%|| 2421/2421 [00:00<00:00, 4413.46it/s]
2024-09-01 04:48:03.270 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cache /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0
fe19306beb1310/random/cache_val/cached_data.pkl
Validation dataset size: 4874
100%|| 2421/2421 [00:00<00:00, 4547.23it/s]
2024-09-01 04:48:03.935 | INFO     | retrieval.datamodule:load_or_cache_data:61 - Saved loaded data to cache /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0
fe19306beb1310/random/cache_pred/cached_data.pkl
Testing dataset size: 4922
Training dataset size after load: 359583
Validation dataset size after load: 4874
Testing dataset size after load: 4922
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python /data/yingzi_ma/lean_project/ReProver/main.py ...
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
2024-09-01 04:48:04.017 | INFO     | __main__:train_test_fisher:730 - Starting progressive training from epoch 0 to 1
2024-09-01 04:48:04.017 | INFO     | __main__:train_test_fisher:744 - Finished progressive training at epoch 0
2024-09-01 04:48:04.018 | WARNING  | __main__:train_test_fisher:754 - No best model found. Using the last trained model.
2024-09-01 04:48:04.019 | INFO     | __main__:train_test_fisher:760 - Testing...
2024-09-01 04:48:04.021 | INFO     | retrieval.main:run_cli:55 - PID: 1562395
[rank: 0] Seed set to 3407
2024-09-01 04:48:04.283 | INFO     | retrieval.main:before_instantiate_classes:28 - Data path: /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310
/random
2024-09-01 04:48:04.283 | INFO     | retrieval.main:before_instantiate_classes:29 - Corpus path: /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb13
10/corpus.jsonl
2024-09-01 04:48:04.841 | INFO     | common:__init__:208 - Building the corpus from /data/yingzi_ma/lean_project/datasets_PT_single_repo_ewc/merged_with_new_mathlib4_2b29e73438e240a427bcecc7c0fe19306beb1310/corpus.jso
nl
/data/yingzi_ma/miniconda3/envs/ReProver/lib/python3.10/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to r
un Lightning on SLURM, prepend your python command with `srun` like so: srun python main.py predict --config retrieval/confs/cli_lean4_r ...
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[rank: 0] Seed set to 3407
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[2024-09-01 04:48:39,715] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 04:48:39,715] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-09-01 04:48:39,715] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
2024-09-01 04:48:42.899 | INFO     | __main__:main:1232 - Running progressive training
2024-09-01 04:48:42.899 | INFO     | __main__:main:1232 - Running progressive training
2024-09-01 04:48:42.899 | INFO     | __main__:main:1232 - Running progressive training
2024-09-01 04:48:42.899 | INFO     | __main__:main:1238 - Configuring LeanDojo...
2024-09-01 04:48:42.899 | INFO     | __main__:main:1238 - Configuring LeanDojo...
2024-09-01 04:48:42.899 | INFO     | __main__:main:1238 - Configuring LeanDojo...
2024-09-01 04:48:42.903 | INFO     | generate_benchmark_lean4:configure_leandojo:317 - Current working directory: /data/yingzi_ma/lean_project/ReProver
2024-09-01 04:48:42.903 | INFO     | generate_benchmark_lean4:configure_leandojo:317 - Current working directory: /data/yingzi_ma/lean_project/ReProver
2024-09-01 04:48:42.903 | INFO     | generate_benchmark_lean4:configure_leandojo:317 - Current working directory: /data/yingzi_ma/lean_project/ReProver
2024-09-01 04:48:42.903 | INFO     | __main__:main:1240 - LeanDojo configured
2024-09-01 04:48:42.904 | INFO     | __main__:main:1240 - LeanDojo configured
2024-09-01 04:48:42.904 | INFO     | __main__:main:1240 - LeanDojo configured
2024-09-01 04:48:43.043 | INFO     | __main__:search_github_repositories:423 - Failed to search GitHub
Found 2 repositories
Using lambda = 0.1
2024-09-01 04:48:43.044 | INFO     | __main__:main:1273 - An error occurred: list index out of range
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/main.py", line 1248, in main
    repo = repos[i]
IndexError: list index out of range
2024-09-01 04:48:43.048 | INFO     | __main__:search_github_repositories:423 - Failed to search GitHub
Found 2 repositories
Using lambda = 0.1
2024-09-01 04:48:43.048 | INFO     | __main__:main:1273 - An error occurred: list index out of range
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/main.py", line 1248, in main
    repo = repos[i]
IndexError: list index out of range
2024-09-01 04:48:43.054 | INFO     | __main__:search_github_repositories:423 - Failed to search GitHub
Found 2 repositories
Using lambda = 0.1
2024-09-01 04:48:43.054 | INFO     | __main__:main:1273 - An error occurred: list index out of range
Traceback (most recent call last):
  File "/data/yingzi_ma/lean_project/ReProver/main.py", line 1248, in main
    repo = repos[i]
IndexError: list index out of range

